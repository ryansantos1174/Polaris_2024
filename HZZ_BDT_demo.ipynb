{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../../images/ATLASOD.gif\" style=\"width:50%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to rediscover the Higgs boson yourself - with a BDT!\n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to apply Machine Learning in search for the Higgs boson!\n",
    "\n",
    "ATLAS Open Data provides open access to proton-proton collision data at the LHC for educational purposes. ATLAS Open Data resources are ideal for high-school, undergraduate and postgraduate students.\n",
    "\n",
    "Notebooks are web applications that allow you to create and share documents that can contain for example:\n",
    "1. live code\n",
    "2. visualisations\n",
    "3. narrative text\n",
    "\n",
    "This notebook builds on [HZZAnalysis.ipynb](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HZZAnalysis.ipynb) in the same folder as this notebook. \n",
    "\n",
    "HZZAnalysis.ipynb loosely follows the [discovery of the Higgs boson by ATLAS](https://www.sciencedirect.com/science/article/pii/S037026931200857X) (mostly Section 4 and 4.1)\n",
    "\n",
    "Notebooks are a perfect platform to develop Machine Learning for your work, since you'll need exactly those 3 things: code, visualisations and narrative text!\n",
    "\n",
    "We're interested in Machine Learning because we can design an algorithm to figure out for itself how to do various analyses, potentially saving us countless human-hours of design and analysis work.\n",
    "\n",
    "Machine Learning use within ATLAS includes: \n",
    "* particle tracking\n",
    "* particle identification\n",
    "* signal/background classification\n",
    "* and more!\n",
    "\n",
    "This notebook will focus on signal/background classification.\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "1. run a Boosted Decision Tree to classify signal and background\n",
    "2. know some things you can change to improve your Boosted Decision Tree\n",
    "\n",
    "Feynman diagram pictures are borrowed from our friends at https://www.particlezoo.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"images/feynman_diagrams/HZZ_feynman.png\" style=\"width:40%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents: \n",
    "\n",
    "[Running a Jupyter notebook](#running) <br />\n",
    "[First time setup on your computer (no need on mybinder)](#setup_computer) <br />\n",
    "[To setup everytime](#setup_everytime) <br />\n",
    "[Lumi, fraction, file path](#fraction) <br />\n",
    "[Samples](#samples) <br />\n",
    "[Changing a cut](#changing_cut) <br />\n",
    "[Applying a cut](#applying_cut) <br />\n",
    "[Optimisation](#optimisation) <br />\n",
    "[Boosted Decision Tree (BDT)](#BDT) <br />\n",
    "  [Training and Testing split](#train_test) <br />\n",
    "  [Training Decision Trees](#training) <br />\n",
    "  [Assessing a Classifier's Performance](#performance) <br />\n",
    "  [Receiver Operating Characteristic (ROC) curve](#ROC) <br />\n",
    "  [Overtraining check](#overtraining) <br />\n",
    "  [Optimisation](#BDT_optimisation) <br />\n",
    "[Going further](#going_further) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='running'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Jupyter notebook\n",
    "\n",
    "To run the whole Jupyter notebook, in the top menu click Cell -> Run All.\n",
    "\n",
    "To propagate a change you've made to a piece of code, click Cell -> Run All Below.\n",
    "\n",
    "You can also run a single code cell, by clicking Cell -> Run Cells, or using the keyboard shortcut Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_computer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ryan/Documents/Coding/Polaris_2024\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First time setup on your computer (no need on mybinder)\n",
    "This first cell only needs to be run the first time you open this notebook on your computer. \n",
    "\n",
    "If you close Jupyter and re-open on the same computer, you won't need to run this first cell again.\n",
    "\n",
    "If you open on mybinder, you don't need to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: uproot3==3.14.4 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (3.14.4)\n",
      "Requirement already satisfied: pandas==1.4.4 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (1.4.4)\n",
      "Requirement already satisfied: numpy==1.26.4 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib==3.8.3 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (3.8.3)\n",
      "Requirement already satisfied: scikit-learn==1.4.1.post1 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: awkward0 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from uproot3==3.14.4) (0.15.5)\n",
      "Requirement already satisfied: uproot3-methods in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from uproot3==3.14.4) (0.10.1)\n",
      "Requirement already satisfied: cachetools in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from uproot3==3.14.4) (5.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from pandas==1.4.4) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from pandas==1.4.4) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from matplotlib==3.8.3) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from matplotlib==3.8.3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from matplotlib==3.8.3) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from matplotlib==3.8.3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from matplotlib==3.8.3) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from matplotlib==3.8.3) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from matplotlib==3.8.3) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from scikit-learn==1.4.1.post1) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from scikit-learn==1.4.1.post1) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from scikit-learn==1.4.1.post1) (3.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ryan/.local/pipx/venvs/jupyterlab/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas==1.4.4) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade --user pip # update the pip package installer\n",
    "!{sys.executable} -m pip install uproot3==3.14.4 pandas==1.4.4 numpy==1.26.4 matplotlib==3.8.3 scikit-learn==1.4.1.post1  # install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup_everytime'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To setup everytime\n",
    "Cell -> Run All Below\n",
    "\n",
    "to be done every time you re-open this notebook.\n",
    "\n",
    "We're going to be using a number of tools to help us:\n",
    "* uproot: lets us read .root files typically used in particle physics into data formats used in Machine Learning\n",
    "* pandas: lets us store data as dataframes, a format widely used in Machine Learning\n",
    "* numpy: provides numerical calculations such as histogramming\n",
    "* matplotlib: common tool for making plots, figures, images, visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import uproot3 # for reading .root files\n",
    "import pandas as pd # to store data as dataframe\n",
    "import time # to measure time to analyse\n",
    "import math # for mathematical functions such as square root\n",
    "import numpy as np # # for numerical calculations such as histogramming\n",
    "import vector\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "\n",
    "import infofile # local file containing info on cross-sections, sums of weights, dataset IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fraction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lumi, fraction, file path\n",
    "\n",
    "General definitions of fraction of data used, where to access the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 10 # fb-1 # data_A+B+C+D\n",
    "\n",
    "fraction = 1 # reduce this is you want the code to run quicker\n",
    "                                                                                                                                  \n",
    "#tuple_path = \"Input/4lep/\" # local \n",
    "tuple_path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/4lep/\" # web address\n",
    "MeV = 0.001\n",
    "GeV = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='samples'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "\n",
    "In this notebook we only process the signal <span style=\"color:blue\">H->ZZ</span> and the main background <span style=\"color:red\">ZZ</span>, for illustration purposes. You can add data and the Z and ttbar <span style=\"color:red\">backgrounds</span> after if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['data_A','data_B','data_C','data_D'],\n",
    "    },\n",
    "\n",
    "    r'Background $Z,t\\bar{t}$' : { # Z + ttbar\n",
    "        'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "        'color' : \"#6b59d3\" # purple\n",
    "    },\n",
    "\n",
    "    r'Background $ZZ^*$' : { # ZZ\n",
    "        'list' : ['llll'],\n",
    "        'color' : \"#ff0000\" # red\n",
    "    },\n",
    "\n",
    "    r'Signal ($m_H$ = 125 GeV)' : { # H -> ZZ -> llll\n",
    "        'list' : ['ggH125_ZZ4lep','VBFH125_ZZ4lep','WH125_ZZ4lep','ZH125_ZZ4lep'],\n",
    "        'color' : \"#00cdff\" # light blue\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to get data from files.\n",
    "\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event, so that processing is quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_files():\n",
    "\n",
    "    data = {} # define empty dictionary to hold dataframes\n",
    "    for s in samples: # loop over samples\n",
    "        print('Processing '+s+' samples') # print which sample\n",
    "        frames = [] # define empty list to hold data\n",
    "        for val in samples[s]['list']: # loop over each file\n",
    "            if s == 'data': prefix = \"Data/\" # Data prefix\n",
    "            else: # MC prefix\n",
    "                prefix = \"MC/mc_\"+str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "            fileString = tuple_path+prefix+val+\".4lep.root\" # file name to open\n",
    "            temp = read_file(fileString,val) # call the function read_file defined below\n",
    "            frames.append(temp) # append dataframe returned from read_file to list of dataframes\n",
    "        data[s] = pd.concat(frames) # dictionary entry is concatenated dataframes\n",
    "    \n",
    "    return data # return dictionary of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define function to get cross-section weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_weight(sample):\n",
    "    info = infofile.infos[sample] # open infofile\n",
    "    xsec_weight = (lumi*1000*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"]) #*1000 to go from fb-1 to pb-1\n",
    "    return xsec_weight # return cross-section weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define function to calculate weight of MC event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(xsec_weight, mcWeight, scaleFactor_PILEUP,\n",
    "                scaleFactor_ELE, scaleFactor_MUON, \n",
    "                scaleFactor_LepTRIGGER ):\n",
    "    return xsec_weight*mcWeight*scaleFactor_PILEUP*scaleFactor_ELE*scaleFactor_MUON*scaleFactor_LepTRIGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add functions to return the individual lepton transverse momenta, in GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lep_pt_i(lep_pt,i):\n",
    "    return lep_pt[i]/1000 # /1000 to go from MeV to GeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='changing_cut'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing a cut\n",
    "\n",
    "We apply 'cuts' to throw away collisions that have properties different to the signal we're looking for.\n",
    "\n",
    "If you change a cut: Cell -> Run All Below\n",
    "\n",
    "If you change a cut here, you also need to make sure the cut is applied in the \"[Applying a cut](#applying_cut)\" cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut on lepton charge\n",
    "# paper: \"selecting two pairs of isolated leptons, each of which is comprised of two leptons with the same flavour and opposite charge\"\n",
    "def cut_lep_charge(lep_charge):\n",
    "# throw away when sum of lepton charges is not equal to 0\n",
    "# first lepton is [0], 2nd lepton is [1] etc\n",
    "    return lep_charge[0] + lep_charge[1] + lep_charge[2] + lep_charge[3] != 0\n",
    "\n",
    "# cut on lepton type\n",
    "# paper: \"selecting two pairs of isolated leptons, each of which is comprised of two leptons with the same flavour and opposite charge\"\n",
    "def cut_lep_type(lep_type):\n",
    "# for an electron lep_type is 11\n",
    "# for a muon lep_type is 13\n",
    "# throw away when none of eeee, mumumumu, eemumu\n",
    "    sum_lep_type = lep_type[0] + lep_type[1] + lep_type[2] + lep_type[3]\n",
    "    return (sum_lep_type != 44) and (sum_lep_type != 48) and (sum_lep_type != 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mllll(lep_pt, lep_eta, lep_phi, lep_E):\n",
    "    # construct awkward 4-vector array\n",
    "    p4 = vector.zip({\"pt\": lep_pt, \"eta\": lep_eta, \"phi\": lep_phi, \"E\": lep_E})\n",
    "    # calculate invariant mass of first 4 leptons\n",
    "    # [:, i] selects the i-th lepton in each event\n",
    "    # .M calculates the invariant mass\n",
    "    return (p4[:, 0] + p4[:, 1] + p4[:, 2] + p4[:, 3]).M * MeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='applying_cut'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a cut\n",
    "If you add a cut: Cell -> Run All Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,sample):\n",
    "    start = time.time() # start the clock\n",
    "    print(\"\\tProcessing: \"+sample) # print which sample is being processed\n",
    "    data_all = pd.DataFrame() # define empty pandas DataFrame to hold all data for this sample\n",
    "    tree = uproot3.open(path)[\"mini\"] # open the tree called mini\n",
    "    numevents = uproot3.numentries(path, \"mini\") # number of events\n",
    "    if 'data' not in sample: xsec_weight = get_xsec_weight(sample) # get cross-section weight\n",
    "    for data in tree.iterate(['lep_charge','lep_type','lep_pt',\n",
    "                              # uncomment these variables if you want to calculate masses \n",
    "                              'lep_eta','lep_phi','lep_E', \n",
    "                              # add more variables here if you make cuts on them \n",
    "                              'mcWeight','scaleFactor_PILEUP',\n",
    "                              'scaleFactor_ELE','scaleFactor_MUON',\n",
    "                              'scaleFactor_LepTRIGGER'\n",
    "                             ], # variables to calculate Monte Carlo weight\n",
    "                             outputtype=pd.DataFrame, # choose output type as pandas DataFrame\n",
    "                             entrystop=numevents*fraction): # process up to numevents*fraction\n",
    "\n",
    "        nIn = len(data.index) # number of events in this batch\n",
    "\n",
    "        if 'data' not in sample: # only do this for Monte Carlo simulation files\n",
    "            # multiply all Monte Carlo weights and scale factors together to give total weight\n",
    "            data['totalWeight'] = np.vectorize(calc_weight)(xsec_weight,\n",
    "                                                            data.mcWeight,\n",
    "                                                            data.scaleFactor_PILEUP,\n",
    "                                                            data.scaleFactor_ELE,\n",
    "                                                            data.scaleFactor_MUON,\n",
    "                                                            data.scaleFactor_LepTRIGGER)\n",
    "\n",
    "        # cut on lepton charge using the function cut_lep_charge defined above\n",
    "        #fail = data[ np.vectorize(cut_lep_charge)(data.lep_charge) ].index\n",
    "        #data.drop(fail, inplace=True)\n",
    "\n",
    "        # cut on lepton type using the function cut_lep_type defined above\n",
    "        #fail = data[ np.vectorize(cut_lep_type)(data.lep_type) ].index\n",
    "        #data.drop(fail, inplace=True)\n",
    "\n",
    "        # return the individual lepton transverse momenta in GeV\n",
    "        data['lep_pt_1'] = np.vectorize(calc_lep_pt_i)(data.lep_pt,1)\n",
    "        data['lep_pt_2'] = np.vectorize(calc_lep_pt_i)(data.lep_pt,2)\n",
    "        data['mllll'] = calc_mllll(data.lep_pt, data.lep_eta, data.lep_phi, data.lep_E)\n",
    "        # dataframe contents can be printed at any stage like this\n",
    "        #print(data)\n",
    "\n",
    "        # dataframe column can be printed at any stage like this\n",
    "        #print(data['lep_pt'])\n",
    "\n",
    "        # multiple dataframe columns can be printed at any stage like this\n",
    "        #print(data[['lep_pt','lep_eta']])\n",
    "\n",
    "        nOut = len(data.index) # number of events passing cuts in this batch\n",
    "        data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n",
    "        elapsed = time.time() - start # time taken to process\n",
    "        print(\"\\t\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)+\"\\t in \"+str(round(elapsed,1))+\"s\") # events before and after\n",
    "    \n",
    "    return data_all # return dataframe containing events passing all cuts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the processing happens (this will take some minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.4\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data samples\n",
      "\tProcessing: data_A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 39,\t nOut: \t39\t in 2.6s\n",
      "\tProcessing: data_B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 156,\t nOut: \t156\t in 2.2s\n",
      "\tProcessing: data_C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 237,\t nOut: \t237\t in 4.3s\n",
      "\tProcessing: data_D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 400,\t nOut: \t400\t in 4.9s\n",
      "Processing Background $Z,t\\bar{t}$ samples\n",
      "\tProcessing: Zee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 898,\t nOut: \t898\t in 6.9s\n",
      "\tProcessing: Zmumu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 684,\t nOut: \t684\t in 4.6s\n",
      "\tProcessing: ttbar_lep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 1031,\t nOut: \t1031\t in 8.6s\n",
      "Processing Background $ZZ^*$ samples\n",
      "\tProcessing: llll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 106415,\t nOut: \t106415\t in 191.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 106415,\t nOut: \t106415\t in 235.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 106415,\t nOut: \t106415\t in 269.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 106415,\t nOut: \t106415\t in 301.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 106415,\t nOut: \t106415\t in 335.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 22204,\t nOut: \t22204\t in 336.8s\n",
      "Processing Signal ($m_H$ = 125 GeV) samples\n",
      "\tProcessing: ggH125_ZZ4lep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 104555,\t nOut: \t104555\t in 437.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 60161,\t nOut: \t60161\t in 440.3s\n",
      "\tProcessing: VBFH125_ZZ4lep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 102038,\t nOut: \t102038\t in 396.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 89088,\t nOut: \t89088\t in 426.9s\n",
      "\tProcessing: WH125_ZZ4lep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 15379,\t nOut: \t15379\t in 26.0s\n",
      "\tProcessing: ZH125_ZZ4lep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9412/3192384880.py:52: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_all.append(data) # append dataframe from this batch to the dataframe for the whole sample\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t nIn: 14485,\t nOut: \t14485\t in 19.7s\n",
      "Time taken: 1283.9s\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # time at start of whole processing\n",
    "data = get_data_from_files() # process all files\n",
    "elapsed = time.time() - start # time after whole processing\n",
    "print(\"Time taken: \"+str(round(elapsed,1))+\"s\") # print total time taken to process every file\n",
    "with open(\"analysis_data2.pkl\", \"wb\") as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='optimisation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "\n",
    "Here we define histograms for the variables that we'll look to optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lep_pt_2 = { # dictionary containing plotting parameters for the lep_pt_2 histogram\n",
    "    # change plotting parameters\n",
    "    'bin_width':1, # width of each histogram bin\n",
    "    'num_bins':13, # number of histogram bins\n",
    "    'xrange_min':7, # minimum on x-axis\n",
    "    'xlabel':r'$lep\\_pt$[2] [GeV]', # x-axis label\n",
    "}\n",
    "\n",
    "lep_pt_1 = { # dictionary containing plotting parameters for the lep_pt_1 histogram\n",
    "    # change plotting parameters\n",
    "    'bin_width':1, # width of each histogram bin\n",
    "    'num_bins':28, # number of histogram bins\n",
    "    'xrange_min':7, # minimum on x-axis\n",
    "    'xlabel':r'$lep\\_pt$[1] [GeV]', # x-axis label\n",
    "}\n",
    "\n",
    "SoverB_hist_dict = {'lep_pt_2':lep_pt_2,'lep_pt_1':lep_pt_1} \n",
    "# add a histogram here if you want it plotted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function to illustrate the optimum cut value on individual variables, based on <span style=\"color:blue\">signal</span> to <span style=\"color:red\">background</span> ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_SoverB(data):\n",
    "    \n",
    "    signal = r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' # which sample is the signal\n",
    "\n",
    "    # *******************\n",
    "    # general definitions (shouldn't need to change)\n",
    "\n",
    "    for x_variable,hist in SoverB_hist_dict.items(): # access the dictionary of histograms defined in the cell above\n",
    "\n",
    "        h_bin_width = hist['bin_width'] # get the bin width defined in the cell above\n",
    "        h_num_bins = hist['num_bins'] # get the number of bins defined in the cell above\n",
    "        h_xrange_min = hist['xrange_min'] # get the x-range minimum defined in the cell above\n",
    "        h_xlabel = hist['xlabel'] # get the x-axis label defined in the cell above\n",
    "    \n",
    "        bin_edges = [ h_xrange_min + x*h_bin_width for x in range(h_num_bins+1) ] # bin limits\n",
    "        bin_centres = [ h_xrange_min+h_bin_width/2 + x*h_bin_width for x in range(h_num_bins) ] # bin centres\n",
    "        \n",
    "        signal_x = data[signal][x_variable] # histogram the signal\n",
    "    \n",
    "        mc_x = [] # define list to hold the Monte Carlo histogram entries\n",
    "\n",
    "        for s in samples: # loop over samples\n",
    "            if s not in ['data', signal]: # if not data nor signal\n",
    "                mc_x = [*mc_x, *data[s][x_variable] ] # append to the list of Monte Carlo histogram entries\n",
    "\n",
    "    \n",
    "    \n",
    "        # *************\n",
    "        # Signal and background distributions\n",
    "        # *************\n",
    "        distributions_axes = plt.gca() # get current axes\n",
    " \n",
    "        mc_heights = distributions_axes.hist(mc_x, bins=bin_edges, color='red', \n",
    "                                             label='Total background',\n",
    "                                             histtype='step', # lineplot that's unfilled\n",
    "                                             density=True ) # normalize to form probability density\n",
    "        signal_heights = distributions_axes.hist(signal_x, bins=bin_edges, color='blue',\n",
    "                                                 label=signal, \n",
    "                                                 histtype='step', # lineplot that's unfilled\n",
    "                                                 density=True, # normalize to form probability density\n",
    "                                                 linestyle='--' ) # dashed line\n",
    "        \n",
    "        distributions_axes.set_xlim( left=bin_edges[0], right=bin_edges[-1] ) # x-limits of the distributions axes\n",
    "        distributions_axes.set_ylabel('Arbitrary units' ) # y-axis label for distributions axes\n",
    "        distributions_axes.set_ylim( top=max(signal_heights[0])*1.3 ) # set y-axis limits\n",
    "        plt.title('Signal and background '+x_variable+' distributions') # add title\n",
    "        distributions_axes.legend() # draw the legend\n",
    "        distributions_axes.set_xlabel( h_xlabel ) # x-axis label\n",
    "        \n",
    "        # Add text 'ATLAS Open Data' on plot\n",
    "        plt.text(0.05, # x\n",
    "                 0.93, # y\n",
    "                 'ATLAS Open Data', # text\n",
    "                 transform=distributions_axes.transAxes, # coordinate system used is that of distributions_axes\n",
    "                 fontsize=13 ) \n",
    "        # Add text 'for education' on plot\n",
    "        plt.text(0.05, # x\n",
    "                 0.88, # y\n",
    "                 'for education', # text\n",
    "                 transform=distributions_axes.transAxes, # coordinate system used is that of distributions_axes\n",
    "                 style='italic',\n",
    "                 fontsize=8 )  \n",
    "    \n",
    "        plt.show() # show the Signal and background distributions\n",
    "    \n",
    "    \n",
    "        # *************\n",
    "        # Signal to background ratio\n",
    "        # *************\n",
    "        plt.figure() # start new figure\n",
    "        SoverB = [] # list to hold S/B values\n",
    "        for cut_value in bin_edges: # loop over bins\n",
    "            signal_weights_passing_cut = sum(data[signal][data[signal][x_variable]>cut_value].totalWeight)\n",
    "            background_weights_passing_cut = 0 # start counter for background weights passing cut\n",
    "            for s in samples: # loop over samples\n",
    "                if s not in ['data', signal]: # if not data nor signal\n",
    "                    background_weights_passing_cut += sum(data[s][data[s][x_variable]>cut_value].totalWeight)\n",
    "            if background_weights_passing_cut!=0: # some background passes cut\n",
    "                SoverB_value = signal_weights_passing_cut/background_weights_passing_cut\n",
    "                SoverB_percent = 100*SoverB_value # multiply by 100 for percentage\n",
    "                SoverB.append(SoverB_percent) # append to list of S/B values\n",
    "        \n",
    "        SoverB_axes = plt.gca() # get current axes\n",
    "        SoverB_axes.plot( bin_edges[:len(SoverB)], SoverB ) # plot the data points\n",
    "        SoverB_axes.set_xlim( left=bin_edges[0], right=bin_edges[-1] ) # set the x-limit of the main axes\n",
    "        SoverB_axes.set_ylabel( 'S/B (%)' ) # write y-axis label for main axes\n",
    "        plt.title('Signal to background ratio for different '+x_variable+' cut values', family='sans-serif')\n",
    "        SoverB_axes.set_xlabel( h_xlabel ) # x-axis label \n",
    "        \n",
    "        plt.show() # show S/B plot\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call our function to illustrate the optimum cut value on individual variables, based on <span style=\"color:blue\">signal</span> to <span style=\"color:red\">background</span> ratio.\n",
    "\n",
    "We're not doing any Machine Learning yet! We're looking at the variables we'll later use for Machine Learning.\n",
    "\n",
    "Let's talk through the lep_pt_2 plots.\n",
    "1. Imagine placing a cut at 7 GeV in the distributions of <span style=\"color:blue\">signal</span> and <span style=\"color:red\">background</span> (1st plot). This means keeping all events above 7 GeV in the <span style=\"color:blue\">signal</span> and <span style=\"color:red\">background</span> histograms. \n",
    "2. We then take the ratio of the number of <span style=\"color:blue\">signal</span> events that pass this cut, to the number of <span style=\"color:red\">background</span> events that pass this cut. This gives us a starting value for S/B (2nd plot). \n",
    "3. We then increase this cut value to 8 GeV, 9 GeV, 10 GeV, 11 GeV, 12 GeV. Cuts at these values are throwing away more <span style=\"color:red\">background</span> than <span style=\"color:blue\">signal</span>, so S/B increases. \n",
    "4. There comes a point around 13 GeV where we start throwing away too much <span style=\"color:blue\">signal</span>, thus S/B starts to decrease. \n",
    "5. Our goal is to find the maximum in S/B, and place the cut there.\n",
    "\n",
    "The same logic applies to lep_pt_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'$H \\\\rightarrow ZZ \\\\rightarrow \\\\ell\\\\ell\\\\ell\\\\ell$'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_SoverB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mplot_SoverB\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     15\u001b[0m bin_edges \u001b[38;5;241m=\u001b[39m [ h_xrange_min \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m*\u001b[39mh_bin_width \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(h_num_bins\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) ] \u001b[38;5;66;03m# bin limits\u001b[39;00m\n\u001b[1;32m     16\u001b[0m bin_centres \u001b[38;5;241m=\u001b[39m [ h_xrange_min\u001b[38;5;241m+\u001b[39mh_bin_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m*\u001b[39mh_bin_width \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(h_num_bins) ] \u001b[38;5;66;03m# bin centres\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m signal_x \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m]\u001b[49m[x_variable] \u001b[38;5;66;03m# histogram the signal\u001b[39;00m\n\u001b[1;32m     20\u001b[0m mc_x \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# define list to hold the Monte Carlo histogram entries\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m samples: \u001b[38;5;66;03m# loop over samples\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '$H \\\\rightarrow ZZ \\\\rightarrow \\\\ell\\\\ell\\\\ell\\\\ell$'"
     ]
    }
   ],
   "source": [
    "plot_SoverB(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [ATLAS Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X), there are a number of numerical cuts applied, not just on lep_pt_1 and lep_pt_2.\n",
    "\n",
    "Imagine having to separately optimise about 7 variables! Not to mention that applying a cut on one variable could change the distribution of another, which would mean you'd have to re-optimise... Nightmare.\n",
    "\n",
    "This is where a Machine Learning algorithm such as a Boosted Decision Tree (BDT) can come to the rescue. A BDT can optimise all variables at the same time.\n",
    "\n",
    "A BDT not only optimises cuts, but can find correlations in many dimensions that will give better signal/background classification than individual cuts ever could.\n",
    "\n",
    "That's the end of the introduction to why one might want to use a BDT. If you'd like to try using one, just keep reading below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BDT'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosted Decision Tree (BDT)\n",
    "\n",
    "Choose variables for use in the BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_BDT = {} # define empty dictionary to hold dataframes that will be used to train the BDT\n",
    "BDT_inputs = ['lep_pt_1','lep_pt_2'] # list of features for BDT\n",
    "for key in data: # loop over the different keys in the dictionary of dataframes\n",
    "    data_for_BDT[key] = data[key][BDT_inputs].copy()\n",
    "data_for_BDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Organise data ready for the BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sklearn data is usually organised                                                                                                                                           \n",
    "# into one 2D array of shape (n_samples x n_features)                                                                                                                             \n",
    "# containing all the data and one array of categories                                                                                                                             \n",
    "# of length n_samples  \n",
    "\n",
    "all_MC = [] # define empty list that will contain all features for the MC\n",
    "for key in data: # loop over the different keys in the dictionary of dataframes\n",
    "    if key!='data': # only MC should pass this\n",
    "        all_MC.append(data_for_BDT[key]) # append the MC dataframe to the list containing all MC features\n",
    "X = np.concatenate(all_MC) # concatenate the list of MC dataframes into a single 2D array of features, called X\n",
    "\n",
    "all_y = [] # define empty list that will contain labels whether an event in signal or background\n",
    "for key in data: # loop over the different keys in the dictionary of dataframes\n",
    "    if key!=r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$' and key!='data': # only background MC should pass this\n",
    "        all_y.append(np.zeros(data_for_BDT[key].shape[0])) # background events are labelled with 0\n",
    "all_y.append(np.ones(data_for_BDT[r'$H \\rightarrow ZZ \\rightarrow \\ell\\ell\\ell\\ell$'].shape[0])) # signal events are labelled with 1\n",
    "y = np.concatenate(all_y) # concatenate the list of lables into a single 1D array of labels, called y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='train_test'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training and Testing split\n",
    "One of the first things to do is split your data into a training and testing set. This will split your data into train-test sets: 67%-33%. It will also shuffle entries so you will not get the first 67% of <span style=\"color:orange\">X</span> for training and the last 33% for testing. This is particularly important in cases where you load all <span style=\"color:blue\">signal</span> events first and then the <span style=\"color:red\">background</span> events.\n",
    "\n",
    "Here we split our data into two independent samples. The split is to create a training and testing set. The first will be used for training the classifier and the second to evaluate its performance.\n",
    "\n",
    "We don't want to test on events that we used to train on, this prevents overfitting to some subset of data so the network would be good for the test data but much worse at any *new* data it sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# make train and test sets\n",
    "X_train,X_test, y_train,y_test = train_test_split(X, y, \n",
    "                                                  test_size=0.33, \n",
    "                                                  random_state=492 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Decision Trees\n",
    "We'll use SciKit Learn (sklearn) in this tutorial. Other possible tools include keras and pytorch. \n",
    "\n",
    "Here we set several hyper-parameters to non default values.\n",
    "\n",
    "After instantiating our <span style=\"color:orange\">AdaBoostClassifier</span>, call the <span style=\"color:orange\">fit()</span> method with the training sample as an argument. This will train the tree, now we are ready to evaluate the performance on the held out testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2) # maximum depth of the tree\n",
    "bdt = AdaBoostClassifier(dt,\n",
    "                        algorithm='SAMME', # SAMME discrete boosting algorithm\n",
    "                        n_estimators=12, # max number of estimators at which boosting is terminated\n",
    "                        learning_rate=0.5) # shrinks the contribution of each classifier by learning_rate\n",
    "\n",
    "start = time.time() # time at start of BDT fit\n",
    "bdt.fit(X_train, y_train) # fit BDT to training set\n",
    "elapsed = time.time() - start # time after fitting BDT\n",
    "print(\"Time taken to fit BDT: \"+str(round(elapsed,1))+\"s\") # print total time taken to fit BDT\n",
    "print(bdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <span style=\"color:orange\">fit()</span> method returns the trained classifier. When printed out all the hyper-parameters are listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='performance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing a Classifier's Performance\n",
    "Next let's create a quick report on how well our classifier is doing. It is important to make sure you use samples not seen by the classifier to get an unbiased estimate of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "y_predicted = bdt.predict(X_test) # get predicted y for test set\n",
    "print (classification_report(y_test, y_predicted,\n",
    "                            target_names=[\"background\", \"signal\"]))\n",
    "print (\"Area under ROC curve for test data: %.4f\"%(roc_auc_score(y_test,\n",
    "                                                    bdt.decision_function(X_test))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate that point, here's the same performance metrics evaluated on the training set instead. The estimates of the performance are more optimistic than on an unseen set of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = bdt.predict(X_train) # get predicted y for train set\n",
    "print (classification_report(y_train, y_predicted,\n",
    "                            target_names=[\"background\", \"signal\"]))\n",
    "print (\"Area under ROC curve for training data: %.4f\"%(roc_auc_score(y_train,\n",
    "                                                    bdt.decision_function(X_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ROC'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operarting Characteristic (ROC) curve for BDT\n",
    "Another useful plot to judge the performance of a classifier is to look at the ROC curve directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first plot the Neural Network output\n",
    "signal_decisions = bdt.decision_function(X[y>0.5]).ravel() # get probabilities on signal\n",
    "background_decisions = bdt.decision_function(X[y<0.5]).ravel() # get decisions on background\n",
    "\n",
    "plt.hist(background_decisions, color='red', label='background', \n",
    "         histtype='step', # lineplot that's unfilled\n",
    "         density=True ) # normalize to form a probability density\n",
    "plt.hist(signal_decisions, color='blue', label='signal', \n",
    "         histtype='step', # lineplot that's unfilled\n",
    "         density=True, # normalize to form a probability density\n",
    "         linestyle='--' ) # dashed line\n",
    "plt.xlabel('BDT output') # add x-axis label\n",
    "plt.ylabel('Arbitrary units') # add y-axis label\n",
    "plt.legend() # add legend\n",
    "\n",
    "\n",
    "# we then plot the ROC\n",
    "plt.figure() # make new figure \n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "decisions = bdt.decision_function(X_test).ravel() # get probabilities on test set\n",
    "\n",
    "# Compute ROC curve and area under the curve\n",
    "fpr, tpr, _ = roc_curve(y_test, # actual\n",
    "                        decisions ) # predicted\n",
    "\n",
    "# Compute area under the curve for training set\n",
    "roc_auc = auc(fpr, # false positive rate \n",
    "              tpr) # true positive rate\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC (area = %0.2f)'%(roc_auc)) # plot test ROC curve\n",
    "plt.plot([0, 1], # x from 0 to 1\n",
    "         [0, 1], # y from 0 to 1\n",
    "         '--', # dashed line\n",
    "         color='grey', label='Luck')\n",
    "\n",
    "plt.xlabel('False Positive Rate') # x-axis label\n",
    "plt.ylabel('True Positive Rate') # y-axis label\n",
    "plt.title('Receiver operating characteristic (ROC) curve') # title\n",
    "plt.legend() # add legend\n",
    "plt.grid() # add grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliding the cut threshold in Neural Network output (upper plot) from right to left builds up the ROC curve (lower plot) from bottom to top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overtraining'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDT Overtraining Check\n",
    "Comparing the BDT's output distribution for the training and testing set is a popular way in HEP to check for overtraining. The <span style=\"color:orange\">compare_train_test()</span> method will plot the shape of the BDT's decision function for each class, as well as overlaying it with the decision function in the training set.\n",
    "\n",
    "There are techniques to prevent overtraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_train_test(clf, X_train, y_train, X_test, y_test):\n",
    "    decisions = [] # list to hold decisions of classifier\n",
    "    for X,y in ((X_train, y_train), (X_test, y_test)): # train and test\n",
    "        d1 = clf.decision_function(X[y<0.5]).ravel() # background\n",
    "        d2 = clf.decision_function(X[y>0.5]).ravel() # signal\n",
    "        decisions += [d1, d2] # add to list of classifier decision\n",
    "    \n",
    "    highest_decision = max(np.max(d) for d in decisions) # get maximum score\n",
    "    bin_edges = [] # list to hold bin edges\n",
    "    bin_edge = -1.1 # start counter for bin_edges\n",
    "    while bin_edge < highest_decision: # up to highest score\n",
    "        bin_edge += 0.1 # increment\n",
    "        bin_edges.append(bin_edge)\n",
    "    \n",
    "    plt.hist(decisions[0], # background in train set\n",
    "             bins=bin_edges, # lower and upper range of the bins\n",
    "             density=True, # area under the histogram will sum to 1\n",
    "             histtype='stepfilled', # lineplot that's filled\n",
    "             color='red', label='B (train)', # Background (train)\n",
    "            alpha=0.5 ) # half transparency\n",
    "    plt.hist(decisions[1], # background in train set\n",
    "             bins=bin_edges, # lower and upper range of the bins\n",
    "             density=True, # area under the histogram will sum to 1\n",
    "             histtype='stepfilled', # lineplot that's filled\n",
    "             color='blue', label='S (train)', # Signal (train)\n",
    "            alpha=0.5 ) # half transparency\n",
    "\n",
    "    hist_background, bin_edges = np.histogram(decisions[2], # background test\n",
    "                                              bins=bin_edges, # number of bins in function definition\n",
    "                                              density=True ) # area under the histogram will sum to 1\n",
    "    \n",
    "    scale = len(decisions[2]) / sum(hist_background) # between raw and normalised\n",
    "    err_background = np.sqrt(hist_background * scale) / scale # error on test background\n",
    "\n",
    "    width = 0.1 # histogram bin width\n",
    "    center = (bin_edges[:-1] + bin_edges[1:]) / 2 # bin centres\n",
    "    \n",
    "    plt.errorbar(x=center, y=hist_background, yerr=err_background, fmt='o', # circles\n",
    "                 c='red', label='B (test)' ) # Background (test)\n",
    "    \n",
    "    hist_signal, bin_edges = np.histogram(decisions[3], # siganl test\n",
    "                                          bins=bin_edges, # number of bins in function definition\n",
    "                                          density=True ) # area under the histogram will sum to 1\n",
    "    scale = len(decisions[3]) / sum(hist_signal) # between raw and normalised\n",
    "    err_signal = np.sqrt(hist_signal * scale) / scale # error on test background\n",
    "    \n",
    "    plt.errorbar(x=center, y=hist_signal, yerr=err_signal, fmt='o', # circles\n",
    "                 c='blue', label='S (test)' ) # Signal (test)\n",
    "    \n",
    "    plt.xlabel(\"BDT output\") # write x-axis label\n",
    "    plt.ylabel(\"Arbitrary units\") # write y-axis label\n",
    "    plt.legend() # add legend\n",
    "    \n",
    "compare_train_test(bdt, X_train, y_train, X_test, y_test) # call compare_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NN_optimisation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BDT Optimisation\n",
    "\n",
    "Here we get the BDT's decision function for every event that was processed at the begininning (so could be data, <span style=\"color:blue\">signal</span>, <span style=\"color:red\">background</span>...). The higher the decision function, the more the BDT thinks that event looks like <span style=\"color:blue\">signal</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = bdt.decision_function(X)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we save the BDT output to our dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_events = 0 # start counter for total number of events for which output is saved\n",
    "for key in data: # loop over samples\n",
    "    data[key]['BDT_output'] = y_predicted[cumulative_events:cumulative_events+len(data[key])]\n",
    "    cumulative_events += len(data[key]) # increment counter for total number of events\n",
    "    print(data[key]['BDT_output']) # print the dataframe column BDT_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define parameters to plot the BDT output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_output = { # dictionary containing plotting parameters for the mllll histogram\n",
    "    # change plotting parameters\n",
    "    'bin_width':0.1, # width of each histogram bin\n",
    "    'num_bins':14, # number of histogram bins\n",
    "    'xrange_min':-1, # minimum on x-axis\n",
    "    'xlabel':'BDT output', # x-axis label\n",
    "}\n",
    "\n",
    "SoverB_hist_dict = {'BDT_output':BDT_output}\n",
    "\n",
    "with open(\"bdt_model.pkl\", \"wb\") as model:\n",
    "    pickle.dump(bdt, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call the function defined above to to illustrate the optimum cut value on BDT output, based on <span style=\"color:blue\">signal</span> to <span style=\"color:red\">background</span> ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_SoverB(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything into a BDT means we only have 1 variable to optimise. The <span style=\"color:blue\">signal</span> and <span style=\"color:red\">background</span> distributions are separated much better when looking at BDT output, compared to individual variables. Cutting on BDT output also achieves much higher S/B values than on individual variables.\n",
    "\n",
    "BDTs can achieve better S/B ratios because they find correlations in many dimensions that will give better signal/background classification.\n",
    "\n",
    "Hopefully you've enjoyed this discussion on optimising for <span style=\"color:blue\">signal</span> to <span style=\"color:red\">background</span> ratio, and in particular how a BDT can be used to facilitate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='going_further'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "If you want to go further, there are a number of things you could try: \n",
    "* Increase the fraction of events that are processed in '[Lumi, fraction, file path](#fraction)'.\n",
    "* Add in the other <span style=\"color:blue\">H->ZZ signal</span> samples in '[Samples](#samples)'. You can copy them from [HZZAnalysis.ipynb](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HZZAnalysis.ipynb). Try adding them one at a time first, then see how things look with all added.\n",
    "* Add in the Z and ttbar <span style=\"color:red\">backgrounds</span> samples in '[Samples](#samples)'. You can copy them from [HZZAnalysis.ipynb](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HZZAnalysis.ipynb). Try adding them separately first, then see how things look with both added.\n",
    "* Add some more variables into the in '[Boosted Decision Tree (BDT)](#BDT)'. Add them in one at a time, rather than all at once, because adding a variable could decrease BDT performance, due to anti-correlation. For some ideas of variables, you can look at the paper for the [discovery of the Higgs boson by ATLAS](https://www.sciencedirect.com/science/article/pii/S037026931200857X) (mostly Section 4 and 4.1).\n",
    "* Add in real data in '[Samples](#samples)' and see whether the BDT output distributions in data and simulation match. You can copy data from [HZZAnalysis.ipynb](https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata/blob/master/13-TeV-examples/uproot_python/HZZAnalysis.ipynb). \n",
    "* Modify some BDT hyper-parameters in '[Training Decision Trees](#training)'.\n",
    "\n",
    "With each change, keep an eye on the:\n",
    "* total area under the ROC curve, \n",
    "* separation between <span style=\"color:blue\">signal</span> and <span style=\"color:red\">background</span> in the BDT output distribution\n",
    "* S/B scores that can be achieved\n",
    "\n",
    "Notice that we've trained and tested our BDT on simulated data. We would then *apply* it to real experimental data. Once you're happy with your BDT, you may want to put it back into a full analysis to run over all data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['data_A','data_B','data_C','data_D'],\n",
    "    },\n",
    "\n",
    "    r'Background $Z,t\\bar{t}$' : { # Z + ttbar\n",
    "        'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "        'color' : \"#6b59d3\" # purple\n",
    "    },\n",
    "\n",
    "    r'Background $ZZ^*$' : { # ZZ\n",
    "        'list' : ['llll'],\n",
    "        'color' : \"#ff0000\" # red\n",
    "    },\n",
    "\n",
    "    r'Signal ($m_H$ = 125 GeV)' : { # H -> ZZ -> llll\n",
    "        'list' : ['ggH125_ZZ4lep','VBFH125_ZZ4lep','WH125_ZZ4lep','ZH125_ZZ4lep'],\n",
    "        'color' : \"#00cdff\" # light blue\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "data = get_data_from_files()\n",
    "data_for_BDT = {} # define empty dictionary to hold dataframes that will be used to train the BDT\n",
    "BDT_inputs = ['lep_pt_1','lep_pt_2'] # list of features for BDT\n",
    "for key in data: # loop over the different keys in the dictionary of dataframes\n",
    "    data_for_BDT[key] = data[key][BDT_inputs].copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_MC = [] # define empty list that will contain all features for the MC\n",
    "for key in data: # loop over the different keys in the dictionary of dataframes\n",
    "    if key!='data': # only MC should pass this\n",
    "        all_MC.append(data_for_BDT[key]) # append the MC dataframe to the list containing all MC features\n",
    "X = np.concatenate(all_MC) # concatenate the list of MC dataframes into a single 2D array of features, called X\n",
    "\n",
    "y_predicted = bdt.decision_function(X)\n",
    "\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_BDT[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['data']\n",
    "with open(\"analysis_data.pkl\", \"wb\") as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"analysis_data.pkl\", \"rb\") as file:\n",
    "    a = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
